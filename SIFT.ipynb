# Matching di immagini tramite SIFT

## 1) Ricerca dei key point:
La ricerca dei key point (punti chiave) è un passaggio fondamentale, che permette di identificare caratteristiche distintive in due immagini, che possono poi essere utilizzate per il matching, l’allineamento (sovrapposizione) delle aree comuni e altre analisi. Uno dei metodi più noti per questa operazione è SIFT (Scale-Invariant Feature Transform), che trova e descrive i punti chiave in modo robusto rispetto a cambiamenti di scala, rotazione e illuminazione.

Un key point è un punto nell'immagine che presenta una variazione significativa dell'intensità luminosa rispetto al suo intorno.
L'obiettivo è quindi quello di trovare punti che siano distintivi e facilmente riconoscibili in entrambe le immagini.

### Inserimento delle librerie
import numpy as np
import cv2 
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import ndimage
from scipy.spatial import distance
from scipy.spatial import distance_matrix
import pickle
import math
import matplotlib.pyplot as plt
import numpy as np
from skimage.io import imread
from skimage.color import rgb2gray
from skimage import img_as_float, img_as_ubyte

cv2.__version__
img_dir = '../../Desktop/Progetto_Percezione/Foto_SIFT/'


### Key Point
# load an example image 
img1 = imread(img_dir + '1.jpg') 
img2 = imread(img_dir + '2.jpg')
gray1 = img_as_ubyte(rgb2gray(img1))
gray2 = img_as_ubyte(rgb2gray(img2))
# show image
plt.figure(figsize=(16,6))
plt.subplot(1,2,1), plt.imshow(img1), plt.axis('off')
plt.subplot(1,2,2), plt.imshow(img2), plt.axis('off')
plt.show()
# La funzione cv2.SIFT_create() di OpenCV è utilizzata per creare un oggetto SIFT 
# un metodo per rilevare e descrivere punti di interesse (feature) in immagini. 
# I parametri specificati nella chiamata determinano il comportamento del rilevatore SIFT.

sift = cv2.SIFT_create(nfeatures = 0, nOctaveLayers = 3, contrastThreshold = 0.1, edgeThreshold = 10, sigma = 2)

#Estrapolazione dei key point per le 2 immagini
kp1 = sift.detect(gray1, None)
kp2 = sift.detect(gray2, None)

#Disegna i key point
imgKP1=cv2.drawKeypoints(gray1,kp1,img1,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
imgKP2=cv2.drawKeypoints(gray2,kp2,img2,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

plt.figure(figsize=(30,10))
plt.subplot(1,2,1), plt.imshow(imgKP1, cmap='gray', vmin=0, vmax=255),  plt.axis('off')
plt.subplot(1,2,2), plt.imshow(imgKP2, cmap='gray', vmin=0, vmax=255), plt.axis('off')
plt.show()
## 2) Calcolo del SIFT
Ogni key point viene associato a una direzione dominante basata sul gradiente locale, per garantire l'invarianza rispetto alla rotazione.
Per ogni key point si calcola un descrittore basato sull’orientamento e sull’intensità dei pixel nel suo intorno.
Il descrittore SIFT è tipicamente un vettore di 128 dimensioni.
### Importo le librerie necessarie
import numpy as np
import cv2 
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import ndimage
from scipy.spatial import distance
from scipy.spatial import distance_matrix
import pickle
import math
import matplotlib.pyplot as plt
import imutils
from skimage.io import imread
from skimage.color import rgb2gray
from skimage import img_as_float, img_as_ubyte
 

cv2.ocl.setUseOpenCL(False)
cv2.__version__
img_dir = '../../Desktop/Progetto_Percezione/Foto_SIFT/'

### Ricerca dei punti comune

 
trainImg = plt.imread(img_dir + '2.jpg')
trainImg_gray = img_as_ubyte(rgb2gray(trainImg))

queryImg = plt.imread(img_dir +  '1.jpg')
queryImg_gray = img_as_ubyte(rgb2gray(queryImg))

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))
ax1.imshow(queryImg, cmap="gray")
ax1.set_xlabel("Query image", fontsize=14)

ax2.imshow(trainImg, cmap="gray")
ax2.set_xlabel("Train image (Image to be transformed)", fontsize=14)

plt.show()

#Funzione per estrapolare i key point e i descrittori
def detectAndDescribe(image):
    descriptor = cv2.SIFT_create()    
    (kps, features) = descriptor.detectAndCompute(image, None)
    
    return (kps, features)

kpsA, featuresA = detectAndDescribe(trainImg_gray)
kpsB, featuresB = detectAndDescribe(queryImg_gray)

# display the keypoints and features detected on both images
fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)
ax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))
ax1.set_xlabel("(Query)", fontsize=14)
ax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))
ax2.set_xlabel("(Image to be Transformed)", fontsize=14)
print('I key point trovati nella prima immagine sono',len(kpsA))
print('I key point trovati nella seconda immagine sono', len(kpsB))
## 3) Matching dei punti chiave

Una volta trovati i key point e i loro descrittori, si può passare al matching tra due immagini:
# La funzione matchKeyPointsBF utilizza il matcher brute-force di OpenCV (cv2.BFMatcher)
# per trovare le migliori corrispondenze tra i descrittori delle feature delle 2 immagini
def matchKeyPointsBF(featuresA, featuresB):

    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True) 
       
    # Cerca i best match
    best_matches = bf.match(featuresA,featuresB)
    
    # La lista dei match viene ordinata in base alla distanza (x.distance) in ordine crescente.
    # I match con distanza più bassa sono considerati più simili e quindi più affidabili.
    rawMatches = sorted(best_matches, key = lambda x:x.distance)
    return rawMatches

matches = matchKeyPointsBF(featuresA, featuresB)
img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,matches[:50], None)
 
fig = plt.figure(figsize=(20,8))
plt.imshow(img3), plt.axis('off')

plt.show()


#Funzione che calcola una matrice di omografia che trasforma un insieme di punti chiave (keypoints) 
# da un'immagine ad un'altra, utilizzando corrispondenze di feature tra le immagini.
def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):
    # convert the keypoints to numpy arrays
    kpsA = np.float32([kp.pt for kp in kpsA])
    kpsB = np.float32([kp.pt for kp in kpsB])
    
    if len(matches) > 4:

        # construct the two sets of points
        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])
        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])
        
        # estimate the homography between the sets of points
        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,
            reprojThresh)

        return (matches, H, status)
    else:
        return None


M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)
if M is None:
    print("Error!")
(matches, H, status) = M

# Apply panorama correction
width = trainImg.shape[1] + queryImg.shape[1]
height = trainImg.shape[0] + queryImg.shape[0]


result = cv2.warpPerspective(trainImg, H, (width, height))
result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg

plt.figure(figsize=(20,10))
plt.imshow(result)

plt.axis('off')
plt.show()
# transform the panorama image to grayscale and threshold it 
gray = img_as_ubyte(rgb2gray(result))
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]

# Finds contours from the binary image
cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = imutils.grab_contours(cnts)

# get the maximum contour area
c = max(cnts, key=cv2.contourArea)

# get a bbox from the contour area
(x, y, w, h) = cv2.boundingRect(c)

# crop the image to the bbox coordinates
result = result[y:y + h, x:x + w]

# show the cropped image
plt.figure(figsize=(20,10))
plt.imshow(result), plt.axis('off')
plt.show()



## Esempi
### Match giusti
# Lista delle immagini da utilizzare
image_pairs = [
    ("m2.jpg", "m1.jpg"),  # Prima coppia
    ("p1.jpg", "p2.jpg"),  # Seconda coppia
    ("1m.jpg", "2m.jpg")   # Terza coppia
]

# Funzione per il processo di matching e stitching
def process_images(img_dir, query_file, train_file):
    # Legge le immagini
    queryImg = plt.imread(img_dir + query_file)
    trainImg = plt.imread(img_dir + train_file)
    
    # Converte in scala di grigi
    queryImg_gray = img_as_ubyte(rgb2gray(queryImg))
    trainImg_gray = img_as_ubyte(rgb2gray(trainImg))
    
    # Rileva keypoint e descrittori
    kpsA, featuresA = detectAndDescribe(trainImg_gray)
    kpsB, featuresB = detectAndDescribe(queryImg_gray)
    
    # Effettua il matching
    matches = matchKeyPointsBF(featuresA, featuresB)
    
    # Verifica se il matching è sufficiente
    if len(matches) < 4:
        print("Pochi match trovati. Immagini troppo diverse!")
        return queryImg, trainImg, None
    
    # Calcola l'omografia
    M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)
    if M is None:
        print("Errore! Non è stato possibile calcolare l'omografia.")
        return queryImg, trainImg, None
    (matches, H, status) = M
    
    # Applica la correzione prospettica
    width = trainImg.shape[1] + queryImg.shape[1]
    height = trainImg.shape[0] + queryImg.shape[0]
    result = cv2.warpPerspective(trainImg, H, (width, height))
    result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg
    
    # Trova contorni e ritaglia il risultato
    gray = img_as_ubyte(rgb2gray(result))
    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]
    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)
    if len(cnts) == 0:
        print("Nessun contorno trovato!")
        return queryImg, trainImg, None
    
    # Trova il contorno più grande e ritaglia
    c = max(cnts, key=cv2.contourArea)
    (x, y, w, h) = cv2.boundingRect(c)
    result = result[y:y + h, x:x + w]
    
    return queryImg, trainImg, result

# Esegue il processo per ogni coppia di immagini
for query_file, train_file in image_pairs:
    queryImg, trainImg, result = process_images(img_dir, query_file, train_file)
    
    # Visualizza i risultati
    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, constrained_layout=False, figsize=(16, 9))
    ax1.imshow(queryImg, cmap="gray")
    ax1.set_xlabel("Query image", fontsize=14)

    ax2.imshow(trainImg, cmap="gray")
    ax2.set_xlabel("Train image (Image to be transformed)", fontsize=14)
    

    ax3.imshow(result, cmap="gray")
    ax3.set_xlabel("Match giusto", fontsize=14)
  

    plt.show()
### No Match
Quando si tenta di unire due immagini che non hanno keypoint corrispondenti sufficienti, il processo di matching e stitching fallisce, generando artefatti visivi o risultati non corretti. Questo può accadere per diversi motivi:

1. Immagini con soggetti completamente diversi:

- Se due immagini rappresentano scene o soggetti molto differenti, gli algoritmi non trovano sufficienti corrispondenze tra i punti di interesse (keypoints).
- Ad esempio, unire un'immagine di due paesaggi completamenti diversi, potrebbe portare a un'immagine "esplosa" o a un risultato non coerente, con una delle due immagini sovrapposta in modo casuale o del tutto assente.

2. Differenze significative nella scala o nel colore:

- Se una delle immagini è a colori e l'altra in scala di grigi (ad esempio un'immagine e la sua versione desaturata), i keypoint possono ancora essere rilevati, ma i descrittori che li caratterizzano saranno diversi, poiché sono basati sulle intensità luminose.
- Anche se il matching venisse forzato, il risultato potrebbe essere una sovrapposizione errata, senza evidenti cambiamenti visivi.

3. Numero insufficiente di punti di corrispondenza:

- Gli algoritmi di stitching richiedono un certo numero di keypoint per calcolare l'omografia (una trasformazione geometrica che allinea le immagini). Se il numero di punti corrispondenti è inferiore alla soglia (tipicamente almeno 4), il calcolo fallisce. In questi casi, l'output mostra una sola immagine, o entrambe con un allineamento casuale.

Effetti visivi dei fallimenti:
Artefatti visivi: Se l'omografia tenta di forzare un allineamento inesistente, l'immagine risultante potrebbe apparire distorta o con "strappi" visivi.
Sovrapposizioni errate: In casi di matching limitato, una delle immagini può essere posizionata in modo scorretto o completamente sovrapposta all'altra.
Output non significativo: In assenza di una relazione geometrica tra le immagini, il risultato potrebbe semplicemente replicare una delle immagini originali.
# Lista delle immagini da utilizzare
image_pairs = [
    ("c2.jpg", "c1.jpg"),  # Prima coppia
    ("o1.jpg", "o2.jpg"),  # Seconda coppia
    ("mi2.jpg", "mi1.jpg")   # Terza coppia
]

# Funzione per il processo di matching e stitching
def process_images(img_dir, query_file, train_file):
    # Legge le immagini
    queryImg = plt.imread(img_dir + query_file)
    trainImg = plt.imread(img_dir + train_file)
    
    # Converte in scala di grigi
    queryImg_gray = img_as_ubyte(rgb2gray(queryImg))
    trainImg_gray = img_as_ubyte(rgb2gray(trainImg))
    
    # Rileva keypoint e descrittori
    kpsA, featuresA = detectAndDescribe(trainImg_gray)
    kpsB, featuresB = detectAndDescribe(queryImg_gray)
    
    # Effettua il matching
    matches = matchKeyPointsBF(featuresA, featuresB)
    
    # Verifica se il matching è sufficiente
    if len(matches) < 4:
        print("Pochi match trovati. Immagini troppo diverse!")
        return queryImg, trainImg, None
    
    # Calcola l'omografia
    M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)
    if M is None:
        print("Errore! Non è stato possibile calcolare l'omografia.")
        return queryImg, trainImg, None
    (matches, H, status) = M
    
    # Applica la correzione prospettica
    width = trainImg.shape[1] + queryImg.shape[1]
    height = trainImg.shape[0] + queryImg.shape[0]
    result = cv2.warpPerspective(trainImg, H, (width, height))
    result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg
    
    # Trova contorni e ritaglia il risultato
    gray = img_as_ubyte(rgb2gray(result))
    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]
    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)
    if len(cnts) == 0:
        print("Nessun contorno trovato!")
        return queryImg, trainImg, None
    
    # Trova il contorno più grande e ritaglia
    c = max(cnts, key=cv2.contourArea)
    (x, y, w, h) = cv2.boundingRect(c)
    result = result[y:y + h, x:x + w]
    
    return queryImg, trainImg, result

# Esegue il processo per ogni coppia di immagini
for query_file, train_file in image_pairs:
    queryImg, trainImg, result = process_images(img_dir, query_file, train_file)
    
    # Visualizza i risultati
    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, constrained_layout=False, figsize=(16, 9))
    ax1.imshow(queryImg, cmap="gray")
    ax1.set_xlabel("Query image", fontsize=14)

    ax2.imshow(trainImg, cmap="gray")
    ax2.set_xlabel("Train image (Image to be transformed)", fontsize=14)
    

    ax3.imshow(result, cmap="gray")
    ax3.set_xlabel("Error Match", fontsize=14)
  

    plt.show()
